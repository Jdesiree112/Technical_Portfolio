# RAG Prompting
Prompt templating is considered the **orchestrator of RAG**, refining how a model interprets the results returned from the RAG pipeline. When designing a prompt template for a library of dynamically applied prompt templates, there are some prompting strategies one needs to be familiar with.

## Overview
This document provides a comprehensive guide to designing effective RAG system prompts. It begins by covering the foundational prerequisites: understanding your context data structure and managing context window allocation. These sections help you assess how RAG outputs are formatted (structured vs. unstructured) and how to allocate token budgets across user input, RAG output, and instructions. The guide then walks through the anatomy of a RAG system prompt, breaking down eight key components—from task definition and intent interpretation to examples, formatting, and closing instructions. It explores the distinction between static and dynamic prompts, offering practical considerations for when to use each approach. Throughout, the document emphasizes an iterative process: start simple, test thoroughly, and refine based on your specific use case and data structure.

## Context Data Structure
Before designing any template to orchestrate RAG, it is important to know the structure of the RAG output and what implications that has on the model intended to interpret it. The model will see the RAG output as it is returned from the RAG pipeline, which may be unstructured natural language, comma-separated values (CSV), or some other structure defined by an established schema. Prior to instructing the model on how to interpret the output, it is important to familiarize yourself with how the output is structured. 

RAG outputs are chunks, or parts, of a greater document. These chunks, typically stored in a vector database, are often designed to include metadata. For example, a database of internal documents may be chunked based on common word processor delimiters (markdown headings, HTML `<h#>`, pilcrow, space, non-breaking space, tab character...) so that the documents are split by headers and come accompanied by metadata defining the author, last modified date, and full document title. 

Some libraries, such as LangChain, have [automated structuring](https://python.langchain.com/docs/concepts/structured_outputs/) for RAG outputs. In the linked example, the output is structured in a JSON format. Most models are trained with JSON data represented in their training data, making this a suitable format for models to easily understand the relationship between returns. Another example of a structured output may be a JSON schema, custom to the respective RAG system you are working with.

Example: 

```json
{
  "query": "What are the health benefits of green tea?",
  "generated_answer": "Green tea is rich in antioxidants and may help improve brain function, support fat loss, and reduce the risk of certain cancers. It also has cardiovascular benefits and may improve insulin sensitivity.",
  "retrieved_documents": [
    {
      "title": "The Health Benefits of Green Tea",
      "author": "Jane Smith",
      "source": "https://www.healthline.com/nutrition/top-10-evidence-based-health-benefits-of-green-tea",
      "publication_date": "2022-03-15",
      "content_excerpt": "Green tea contains a catechin called EGCG, which is a natural antioxidant that helps prevent cell damage...",
      "relevance_score": 0.93
    },
    {
      "title": "Green Tea: Benefits and Risks",
      "author": "Dr. Alex Chan",
      "source": "https://www.medicalnewstoday.com/articles/green-tea",
      "publication_date": "2021-11-22",
      "content_excerpt": "Drinking green tea has been associated with a reduced risk of heart disease and improved metabolism...",
      "relevance_score": 0.89
    }
  ],
  "confidence_score": 0.87
}
```

### Explanation of Example Fields

| Field                 | Description                                                                          |
| --------------------- | ------------------------------------------------------------------------------------ |
| `query`               | The user's original question or prompt.                                              |
| `generated_answer`    | The final answer generated by the RAG model.                                         |
| `retrieved_documents` | An array of documents retrieved from the external knowledge base.                    |
| `title`               | Title of the retrieved document.                                                     |
| `author`              | Author of the document (if available).                                               |
| `source`              | URL or source identifier of the document.                                            |
| `publication_date`    | Date when the document was published.                                                |
| `content_excerpt`     | A short excerpt or snippet from the document.                                        |
| `relevance_score`     | Score showing how relevant the document is to the query (typically between 0 and 1). |
| `confidence_score`    | Score reflecting the confidence of the generated answer.                             |

RAG prompt templates can greatly improve the overall response in many ways by applying instructions defining conditional behavior based on metadata, referencing specific metadata, using chunk awareness to contain context, and weighing confidence scoring against relevance. In the **instruction body** section of the prompt template, these powerful insights can be applied. For example, you may want to limit the model's use of RAG to only outputs with a confidence score above a set threshold to eliminate irrelevant returns. You could also consider applying conditional rules that filter results by author or cause the model to ignore returns from documents that have not been modified within a specified time limit.

Templating for unstructured natural language outputs, such as raw chunks split by word processor delimiters, can be filtered by section headings. For example, preceding the filtering instructions, the model could be guided through intent interpretation of the user's raw prompt input. The model could then be instructed to utilize the resulting interpretation to select chunks that align with the user's goal or intentions.

## Context Window Allocation
When crafting a prompt template for use in RAG, you will need to know the input token limit (context window), the anticipated token range for user input, and the anticipated token amount of the RAG output. From here, you will need to determine the token range you can utilize for instructions. Many LLMs have a large token limit, while specialized models taken from Hugging Face vary.

### Examples
*Limits as of 10/2025*

| Model Variant                 | Context Window (Max Tokens) | Approx. Word Count   |
| ----------------------------- | --------------------------- | -------------------- |
| **GPT-4o (OpenAI)**           | 128,000 tokens              | ~96,000 words        |
| **GPT-4-turbo (OpenAI)**      | 128,000 tokens              | ~96,000 words        |
| **GPT-3.5 (OpenAI)**          | 4,096 tokens                | ~3,000 words         |
| **Claude 3 Opus (Anthropic)** | 200,000 tokens              | ~150,000 words       |
| **Claude 3 Sonnet**           | 200,000 tokens              | ~150,000 words       |
| **Claude 3 Haiku**            | 200,000 tokens              | ~150,000 words       |
| **Gemini 1.5 Pro (Google)**   | 1,000,000 tokens            | ~750,000 words       |
| **Gemini 1.0 Pro**            | 32,000 tokens               | ~24,000 words        |
| **Phi-3 (Microsoft)**         | 4,000 to 128,000 tokens*    | ~3,000–96,000 words  |
| **Qwen-2 (Alibaba)**          | 32,000 to 128,000 tokens*   | ~24,000–96,000 words |

*Always check current limits for the model in use*

Per best practices, RAG pipelines generally have a token limit applied to the output. Knowing this limit is helpful because it aids us in allocating space in the prompt template for instructions/guidance and setting an allowance for raw user input. 

### Token Control Methods to Be Aware Of

| Strategy                           | Description                                                                           |
| ---------------------------------- | ------------------------------------------------------------------------------------- |
| **Top-k limit**                    | Only the top **k most relevant chunks** are returned (e.g., top 3 or 5)               |
| **Token budget**                   | Chunks are **accumulated until a token threshold** is reached (e.g., 3,000 tokens)    |
| **Truncation**                     | Chunks longer than a per-chunk token limit (e.g., 500 tokens) are truncated           |
| **Summarization**                  | If content is too long, it's compressed before inclusion                              |
| **Re-ranking with cost-awareness** | Some advanced RAG systems re-rank retrieved docs based on both **relevance and size** |

### Examples from Popular Frameworks

**LangChain**
- Uses `max_tokens_limit` when constructing context.
- Lets you set `chunk_size` and `chunk_overlap` to control indexing and retrieval size.
- Often combined with `Stuff`, `MapReduce`, or `Refine` chains — all have context length limits.

**LlamaIndex**
- Lets you set `input_size` and `num_outputs` per node.
- Has built-in truncation and summarization steps.

**Haystack**
- You can configure how many chunks are returned (`top_k`) and often filter/score based on token count.

### Practical Token Limits for RAG Output

| Model Context Size | Typical RAG Output Budget |
| ------------------ | ------------------------- |
| 4k                 | ~500–1,000 tokens         |
| 16k                | ~2,000–4,000 tokens       |
| 32k                | ~4,000–8,000 tokens       |
| 128k               | ~10,000–30,000 tokens     |
| 200k+              | ~50,000+ tokens possible  |

The above are just some examples of how RAG outputs may be contained that you may need to be aware of to make an informed decision on context window allocation and mindfulness of potential limitations on chunk context. To determine the typical token count range of a RAG system, you can review functions or parameters in the RAG pipeline if you are familiar with how to interpret them, or communicate with co-developers who are. 

Of the steps in my process for designing prompt templates for RAG orchestration, this step is the most conditional. It IS important to check if context window limitations apply, but this will not always be something that will impact or limit templates. This step is mostly important on projects where the model has a small or moderate context window, the RAG output is expected to be fairly large, and the raw user input may be significant (allowing PDF uploads, long user input limits).

## RAG System Prompt Design
The structure, or anatomy, of a RAG system prompt is important as even the ordering of items or formatting **will** influence the output. I typically follow a general order and process to optimize performance. 

### Structure
**Basic Structure**<br>
[task line]<br>
{user_input_variable}<br>
[Intent Interpretation Sub-Task]<br>
[Instruction Body]<br>
{RAG_Output}<br>
[Examples]<br>
[Formatting]<br>
[Closing Instructions]<br>

1. Task Line
The task in any prompt is the most basic and integral part of any prompt. It is critical that the task line clearly defines exactly what the model's goal is for the given task. This is also the ideal location to embed any persona guidance, as it is at the beginning and will refine all processes following this line.

2. User Input
In the above template example, I placed a variable for the user's input following the task but preceding instructions because RAG system instructions generally require some intentional interpretation to refine the output. I have it written as it would be in Python if you were templating using f-strings, but this can also be done through methods such as LangChain prompting, where the user response is passed as an argument. By personal preference, I tend to opt for f-strings when orchestrating an AI application using LangChain, as I have done with my portfolio demo [Mimir](https://github.com/Jdesiree112/Technical_Portfolio/tree/main/CaseStudy_Mimir), because it allows for more control over the placement of items within the template.

3. Intent Interpretation Sub-Task
I include this sub-task line in my RAG system prompts in cases where I expect there may be vague or ambiguous user input, which is nearly always the case with a chatbot that allows free text entry. This task line can be integrated, like in the above example, or tasked to a small model agent, taking an agentic approach to delegate the task to be completed in a pre-processing pipeline. 

There are benefits and drawbacks to the two approaches:

| | Sub-Tasking | Agentic Offloading |
| :-: | :-: | :-: |
| Benefits | Reduced system complexity, technical debt, and centralization of RAG prompting. This also may aid in model understanding as the process will contribute to reasoning processes down the line, allowing for consideration of the intention analysis process and raw input. | Lowered burden on the main response agent, allowing for the intention analysis to replace the sub-task in the prompt template. |
| Drawbacks | Increased prompt complexity. | Increased technical debt. If a separate, specialized model is used, this adds additional memory usage to the application to hold the separate model.  |

For the purpose of this documentation, I will continue with my explanation as though the template is the chosen method. Intent interpretation prompting is a method of specialized instruction that tasks an LLM with analyzing a piece of text to determine the underlying purpose or goal. This is done as such:

- **Role Assignment**: Assign the role of something like an intent classifier exclusively to the sub-task, making it clear that this role only applies to this specific process.
- **Explicit Instructions**: Clearly state the specific task for the AI to perform, which in this case would be to interpret the user's intent or goal.
- **Predefined Intent List**: In a system where the goal or objectives can be predicted to be a set amount, explicitly setting options for the intent improves consistency, security, safety, and accuracy. In systems where the user's needs can be nearly anything, you may want to add content or category flags for harmful requests with explicit instructions on how to handle such requests.
- **Optional - Few-Shot Examples**: Giving examples of user queries with good outputs can teach the model to perform the sub-task more effectively, giving a standard for the model to imitate.
- **Output**: This, unless it needs to be displayed in the output, is generally an invisible process. For a typical system prompt, I would specify that the model is to frame the user's request as it is interpreted for the purposes of completing the overall task.

4. Instruction Body
This segment of the prompt template is where explicit instruction, processes, guidance, and constraints are placed. Some prompting parameters and strategies that may be considered, depending on **data structure**, **task scope**, and **task complexity**, may include:
- **Chain-of-Thought (CoT)**: Breaking the given task into logical atomic steps with clear instructions on how to process each "thinking" step. This improves accuracy in complex, multi-step tasks.
- **Tree-of-Thought (ToT)**: If the overall task is classification or highly conditional with a wide range of possible conditions, I may opt to implement a tree-of-thought rather than a chain-of-thought to guide the model. A ToT is similar to a decision classification tree, where there is a series of questions with two paths determined by possible answers.
- **Multi-hop Reasoning**: This is helpful when the task requires reasoning to a high degree, prompting the model to break complex questions into smaller questions, retrieve context for each of the smaller questions, then build the final response by combining the results.
- **Inclusion Parameters**: I may include inclusion parameters to define what content to pull from the returned RAG output or to refine what is to be included in the response to the user.
- **Exclusion Parameters**: Like with inclusion parameters, I may **exclude** certain conclusions, thinking processes, reasoning, or rationale, and/or returned data from being presented in the final response.
- **Data Interpretation Instructions**: If the data returned is organized with certain delimiters or key terms, I may reference these common delimiters to help the model pull information more efficiently.
- **Relevancy Scoring**: This is a strategy I may implement either as part of a defined thinking process (CoT) or by itself. Relevancy scoring asks the model to reorder RAG outputs by relevancy to the user's request.

**NOTE**: This is a non-exhaustive list. Depending on the specific task, many other prompting patterns could be applied to refine results. By no means would all be applied at once. It is best to **start simple and iterate** to refine the prompt. It is easier to start small and build, as the more complex a prompt is, the higher the error rate will be after a certain point.

## Static vs. Dynamic Prompts
Depending on the goal of the application, RAG system instructions can be either **static** or **dynamic**. 

Static prompts are designed to produce specific types of responses in a narrow or moderate scope. If the purpose of the system is to perform a repetitive task, then a single, static prompt may be favorable. An example may be a model that performs a classification or summarization task, where it pulls from user feedback forms for a QA team to review. This task could be accomplished with a single template where the only dynamic parts are the RAG output and user input.

Dynamic prompts are designed to adapt based on the user input. My [demo](https://github.com/Jdesiree112/Technical_Portfolio/tree/main/CaseStudy_Mimir) is an example of utilizing dynamically built prompts. In these types of prompt templates, parts are interchangeable to adapt based on the user's needs, tailoring to a wide range of scenarios. In my demo, I built an extensive prompt library for different goals and use cases within the scope of the chatbot. A base personality prompt segment is applied by default, and all other prompts are built dynamically through an agentic pipeline. I collected the names of the applicable response agent prompts into a Python dictionary, defaulting each to `False` at the start of each turn and updating needed prompts to `True` if deemed so. The Python dictionary is used in a step just prior to the response generation, where the prompt segments are programmatically joined into an instruction body based on the `True` and `False` values associated. I opted to do this because the chatbot's goals and overall task are likely to be different nearly every turn, where it may be trying to uncover user needs through discovery questions, constructing instructional content, or producing full-length practice tests for the user. Since output quality generally deteriorates as instructions become denser, a dynamic approach is demanded to maintain quality with the level of variation in task objectives. While my demo does not use RAG, the same idea applies. If the RAG output may be used in many different tasks where a significant amount of instructions only apply to some possible user requests, then dynamic templates may be required. An example in RAG output may be a RAG system that is built into a chatbot used internally, connecting to the company's internal documents on a service such as Google Cloud or AWS. Say, perhaps that chatbot is used to pull documents for many tasks where the output needs vary from CSV to natural language. In a case like this, the template will likely need to dynamically apply output formatting instructions.

5. RAG Output
In an f-string prompt template, I would generally place the output following the instruction body to ensure it is interpreted using the instructions I include. It is noteworthy that some libraries, such as LangChain, allow for RAG output to be passed as an argument along with the user input and system instruction. Once again, I simply prefer f-string templating for the granular control of object placement.

6. Examples
If the output is somewhat predictable, such as with static prompt templates with specific output expectations, including examples of golden responses can be highly beneficial if done correctly. Examples must adhere to all rules outlined in the instructions and truly reflect perfect responses.

Examples can be applied for dynamically adapting prompt templates, but must be created for each possible variation and applied dynamically.

The decision to include examples depends on the use case. For static prompts, I will more than likely apply examples (few-shot prompting) by default to teach the model how to combine the instructions given. For dynamic prompts, the additional technical debt is more heavily considered as the dynamic application requires additional logical processes, such as conditional selection based on which prompt segments are in effect. I would consider examples for specific cases in the range of possibilities if quality checks uncover issues and prompt refinement fails to resolve the issue.

7. Formatting
Following either the RAG output, instructions, or examples, the formatting will be defined. This is where you'd define if the response should be in natural language, a structured output like JSON or CSV, or something else. For natural language, this is where you'd apply rules such as heading use, content organization, markdown, LaTeX, HTML, or KaTeX formatting for rendering and so on.

8. Closing Instructions
Closing instructions are important when dealing with more complex tasks. Here, I'd apply instructions for the model to review its response and reinforce key requirements.

### Recap
This is an exhaustive overview aiming to capture many potential parts of a RAG system prompt, noting considerations when relevant. I'd like to reiterate that it is best to start **simple** and **iterate**. Plan prior to writing the system prompt, taking into consideration context window limitations, RAG pipeline structure, output data structuring, and the scope of the application. Communication is key; coordinate with engineering and team leads to ensure that what you are producing aligns with the goals of the application. If a user story is available, reference it to ground your process. Once you have a template that looks like it would deliver, test it and refine from there. It is okay if the first or even second pass is not perfect; this process is iterative.
